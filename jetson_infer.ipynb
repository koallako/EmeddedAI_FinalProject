{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from agemodel import AgePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_model_metrics(model):\n",
    "    \"\"\"모델의 실제 파라미터 수와 FLOPs 계산\"\"\"\n",
    "    # 0이 아닌 파라미터만 카운트\n",
    "    total_params = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "   \n",
    "    def count_conv2d_flops(layer, input_shape):\n",
    "        \"\"\"Convolution layer의 실제 FLOPs 계산\"\"\"\n",
    "        batch_size, in_c, h, w = input_shape\n",
    "        out_c, in_c, k_h, k_w = layer.weight.shape\n",
    "        out_h = h // layer.stride[0]\n",
    "        out_w = w // layer.stride[1]\n",
    "       \n",
    "        # 0이 아닌 weight의 수만 계산\n",
    "        non_zero_weights = torch.count_nonzero(layer.weight).item()\n",
    "        kernel_size = k_h * k_w\n",
    "        weights_per_kernel = in_c * kernel_size\n",
    "        effective_kernels = non_zero_weights / weights_per_kernel\n",
    "       \n",
    "        # 각 출력 픽셀당 MAC(multiply-accumulate) 연산 수 계산\n",
    "        return batch_size * out_h * out_w * non_zero_weights * 2 \n",
    "\n",
    "    def count_linear_flops(layer):\n",
    "        \"\"\"Linear layer의 실제 FLOPs 계산\"\"\"\n",
    "        # 0이 아닌 weight의 수만 계산\n",
    "        non_zero_weights = torch.count_nonzero(layer.weight).item()\n",
    "        return non_zero_weights * 2  \n",
    "\n",
    "    total_flops = 0\n",
    "    input_shape = (1, 3, 64, 64)\n",
    "   \n",
    "    x_shape = input_shape\n",
    "    for layer in model.features:\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            total_flops += count_conv2d_flops(layer, x_shape)\n",
    "            x_shape = (x_shape[0], layer.out_channels,\n",
    "                      x_shape[2]//layer.stride[0],\n",
    "                      x_shape[3]//layer.stride[1])\n",
    "        elif isinstance(layer, torch.nn.MaxPool2d):\n",
    "            x_shape = (x_shape[0], x_shape[1],\n",
    "                      x_shape[2]//2, x_shape[3]//2)\n",
    "\n",
    "   \n",
    "    for layer in model.classifier:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            total_flops += count_linear_flops(layer)\n",
    "\n",
    "    return {\n",
    "        'params': total_params,\n",
    "        'flops': total_flops\n",
    "    }\n",
    "\n",
    "def gstreamer_pipeline(\n",
    "    sensor_id=0,\n",
    "    capture_width=640,\n",
    "    capture_height=480,\n",
    "    display_width=640,\n",
    "    display_height=480,\n",
    "    framerate=15,\n",
    "    flip_method=0,\n",
    "):\n",
    "    return (\n",
    "        f\"nvarguscamerasrc sensor-id={sensor_id} ! \"\n",
    "        f\"video/x-raw(memory:NVMM), \"\n",
    "        f\"width=(int){capture_width}, height=(int){capture_height}, \"\n",
    "        f\"format=(string)NV12, framerate=(fraction){framerate}/1 ! \"\n",
    "        f\"nvvidconv flip-method={flip_method} ! \"\n",
    "        f\"video/x-raw, width=(int){display_width}, height=(int){display_height}, \"\n",
    "        f\"format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_age_group(age):\n",
    "    \"\"\"연령층 변환 함수\"\"\"\n",
    "    age = int(age)\n",
    "    if age <= 70:\n",
    "        return \"young group\"\n",
    "    else:\n",
    "        return \"old group\"\n",
    "\n",
    "def benchmark_and_demo(model_path, duration=30):\n",
    "    \"\"\"실시간 데모와 벤치마크를 동시에 수행\"\"\"\n",
    "    print(f\"\\nRunning demo and benchmark for {os.path.basename(model_path)}...\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    is_student = 'student' in model_path.lower()\n",
    "    model = AgePredictor(is_student=is_student)\n",
    "    \n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            scale_params = checkpoint.get('scale_params', {'min_age': 0, 'max_age': 100})\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            scale_params = {'min_age': 0, 'max_age': 100}\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        scale_params = {'min_age': 0, 'max_age': 100}\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # 모델 메트릭 계산\n",
    "    metrics = calculate_model_metrics(model)\n",
    "    \n",
    "    # 카메라 및 face detector 설정\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(gstreamer_pipeline(), cv2.CAP_GSTREAMER)\n",
    "    \n",
    "    # 측정을 위한 변수들\n",
    "    processing_times = []\n",
    "    inference_times = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nRunning real-time demo...\")\n",
    "        print(f\"Collecting data for {duration} seconds...\")\n",
    "        print(\"Press 'q' to quit early\")\n",
    "        \n",
    "        while True:\n",
    "            if time.time() - start_time >= duration:\n",
    "                break\n",
    "                \n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_start_time = time.time()\n",
    "            \n",
    "            # 얼굴 검출\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "            \n",
    "            # 실시간 정보 표시\n",
    "            elapsed_time = time.time() - start_time\n",
    "            cv2.putText(frame, f\"Time: {elapsed_time:.1f}/{duration}s\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                face_roi = frame[y:y+h, x:x+w]\n",
    "                face_roi = cv2.resize(face_roi, (64, 64))\n",
    "                face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "                face_array = face_rgb.astype(np.float32) / 255.0\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                face_normalized = (face_array - mean) / std\n",
    "                face_tensor = torch.from_numpy(face_normalized.transpose(2, 0, 1)).float().unsqueeze(0)\n",
    "                \n",
    "                # 나이 예측\n",
    "                inference_start = time.time()\n",
    "                with torch.no_grad():\n",
    "                    prediction = model(face_tensor)\n",
    "                inference_time = time.time() - inference_start\n",
    "                inference_times.append(inference_time)\n",
    "                \n",
    "                predicted_age = prediction.item() * (scale_params['max_age'] - \n",
    "                                                   scale_params['min_age']) + scale_params['min_age']\n",
    "                age_group = get_age_group(predicted_age)\n",
    "                \n",
    "                # 결과 표시\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Group: {age_group}\", (x, y-30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            process_time = time.time() - frame_start_time\n",
    "            processing_times.append(process_time)\n",
    "            \n",
    "            cv2.imshow('Age Prediction', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    if processing_times and inference_times:\n",
    "        results = {\n",
    "            'model_size': {\n",
    "                'parameters': metrics['params'],\n",
    "                'size_mb': metrics['params'] * 4 / (1024 * 1024)\n",
    "            },\n",
    "            'flops': metrics['flops'],\n",
    "            'speed': {\n",
    "                'average_process_time_ms': np.mean(processing_times) * 1000,\n",
    "                'average_inference_time_ms': np.mean(inference_times) * 1000\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{os.path.basename(model_path)} Results:\")\n",
    "        print(f\"Parameters: {results['model_size']['parameters']:,}\")\n",
    "        print(f\"Model Size: {results['model_size']['size_mb']:.2f} MB\")\n",
    "        print(f\"FLOPs: {results['flops']:,}\")\n",
    "        print(f\"Average Process Time: {results['speed']['average_process_time_ms']:.2f} ms\")\n",
    "        print(f\"Average Inference Time: {results['speed']['average_inference_time_ms']:.2f} ms\")\n",
    "        \n",
    "        return results\n",
    "    return None\n",
    "\n",
    "def plot_comparison(results):\n",
    "    \"\"\"벤치마크 결과 시각화\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 1. 모델 크기\n",
    "    plt.subplot(131)\n",
    "    params = [results[m]['model_size']['parameters'] for m in results]\n",
    "    plt.bar(results.keys(), params)\n",
    "    plt.title('Model Size (Parameters)')\n",
    "    plt.ylabel('Number of Parameters')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 2. FLOPs\n",
    "    plt.subplot(132)\n",
    "    flops = [results[m]['flops'] for m in results]\n",
    "    plt.bar(results.keys(), flops)\n",
    "    plt.title('Computational Cost (FLOPs)')\n",
    "    plt.ylabel('FLOPs')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 3. 실행 속도\n",
    "    plt.subplot(133)\n",
    "    process_times = [results[m]['speed']['average_process_time_ms'] for m in results]\n",
    "    inference_times = [results[m]['speed']['average_inference_time_ms'] for m in results]\n",
    "    \n",
    "    x = np.arange(len(results))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, process_times, width, label='Process Time')\n",
    "    plt.bar(x + width/2, inference_times, width, label='Inference Time')\n",
    "    plt.title('Processing Speed')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.xticks(x, results.keys(), rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison_ts.png')\n",
    "    plt.close()\n",
    "\n",
    "def run_benchmarks():\n",
    "    \"\"\"전체 벤치마크 실행\"\"\"\n",
    "    model_paths = {\n",
    "        'Teacher': 'teacher_model_best (1).pth',\n",
    "        'Student': 'student_model_best (1).pth',\n",
    "        'Pruned (0.3)': 'age_model_pruned_30.pth',\n",
    "        'Pruned (0.5)': 'age_model_pruned_50.pth',\n",
    "        'Pruned (0.7)': 'age_model_pruned_70.pth'\n",
    "        \n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    print(\"\\nStarting benchmark for each model...\")\n",
    "    \n",
    "    for name, path in model_paths.items():\n",
    "        print(f\"\\nTesting {name} model...\")\n",
    "        results[name] = benchmark_and_demo(path)\n",
    "        \n",
    "        if results[name]:\n",
    "            print(f\"\\n{name} Model Results:\")\n",
    "            print(f\"Parameters: {results[name]['model_size']['parameters']:,}\")\n",
    "            print(f\"Model Size: {results[name]['model_size']['size_mb']:.2f} MB\")\n",
    "            print(f\"FLOPs: {results[name]['flops']:,}\")\n",
    "            print(f\"Average Process Time: {results[name]['speed']['average_process_time_ms']:.2f} ms\")\n",
    "            print(f\"Average Inference Time: {results[name]['speed']['average_inference_time_ms']:.2f} ms\")\n",
    "            \n",
    "            # 결과 저장\n",
    "            with open(f'results_{name.lower()}.json', 'w') as f:\n",
    "                json.dump(results[name], f, indent=4)\n",
    "    \n",
    "    if results:\n",
    "        # 전체 결과 저장\n",
    "        with open('benchmark_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time demo and benchmark comparison...\n",
      "\n",
      "Testing Teacher model...\n",
      "\n",
      "Running demo and benchmark for teacher_model_best (1).pth...\n",
      "\n",
      "Running real-time demo...\n",
      "Press SPACE to capture benchmark sample\n",
      "Press 'q' to quit\n",
      "\n",
      "Benchmark sample 1/1:\n",
      "  Process Time: 330.2ms\n",
      "  Inference Time: 47.6ms\n",
      "  Age Group: old group\n",
      "\n",
      "Benchmark samples collected!\n",
      "\n",
      "Teacher Model Results:\n",
      "Parameters: 2,190,913\n",
      "Model Size: 8.36 MB\n",
      "FLOPs: 86,770,176\n",
      "Average Process Time: 330.18 ms\n",
      "Average Inference Time: 47.59 ms\n",
      "\n",
      "Testing Student model...\n",
      "\n",
      "Running demo and benchmark for student_model_best (1).pth...\n",
      "\n",
      "Running real-time demo...\n",
      "Press SPACE to capture benchmark sample\n",
      "Press 'q' to quit\n",
      "\n",
      "Benchmark sample 1/1:\n",
      "  Process Time: 287.1ms\n",
      "  Inference Time: 29.1ms\n",
      "  Age Group: old group\n",
      "\n",
      "Benchmark samples collected!\n",
      "\n",
      "Student Model Results:\n",
      "Parameters: 1,072,673\n",
      "Model Size: 4.09 MB\n",
      "FLOPs: 24,510,976\n",
      "Average Process Time: 287.07 ms\n",
      "Average Inference Time: 29.12 ms\n",
      "\n",
      "================================================================================\n",
      "Final Comparison Summary\n",
      "================================================================================\n",
      "Model      Size(MB)   Parameters      FLOPs           Process(ms)     Inference(ms)\n",
      "--------------------------------------------------------------------------------\n",
      "Teacher    8.36       2,190,913       86,770,176      330.18          47.59\n",
      "Student    4.09       1,072,673       24,510,976      287.07          29.12\n",
      "\n",
      "Benchmark completed! Results saved to files.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting real-time demo and benchmark comparison...\")\n",
    "    results = run_benchmarks()\n",
    "    print(\"\\nBenchmark completed! Results saved to files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat: 연속 프레임 분석을 통한 안정성 평가 개선\n",
    "\n",
    "- 단일 프레임 벤치마크를 30초 연속 평가 방식으로 변경\n",
    "- 새로운 안정성 지표 추가 (탐지율, 예측 안정성)\n",
    "- 포괄적인 성능 그래프로 시각화 기능 강화\n",
    "- 실시간 FPS 및 경과 시간 모니터링 추가\n",
    "- 에러 처리 및 리소스 정리 개선\n",
    "\n",
    "이 변경으로 단일 캡처가 아닌 연속된 프레임을 분석함으로써 젯슨 나노 배포를 위한 더 신뢰할 수 있는 성능 지표를 제공합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
