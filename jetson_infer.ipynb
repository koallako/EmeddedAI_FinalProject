{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from agemodel import AgePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_model_metrics(model):\n",
    "    \"\"\"모델의 실제 파라미터 수와 FLOPs 계산\"\"\"\n",
    "    # 0이 아닌 파라미터만 카운트\n",
    "    total_params = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "   \n",
    "    def count_conv2d_flops(layer, input_shape):\n",
    "        \"\"\"Convolution layer의 실제 FLOPs 계산\"\"\"\n",
    "        batch_size, in_c, h, w = input_shape\n",
    "        out_c, in_c, k_h, k_w = layer.weight.shape\n",
    "        out_h = h // layer.stride[0]\n",
    "        out_w = w // layer.stride[1]\n",
    "       \n",
    "        # 0이 아닌 weight의 수만 계산\n",
    "        non_zero_weights = torch.count_nonzero(layer.weight).item()\n",
    "        kernel_size = k_h * k_w\n",
    "        weights_per_kernel = in_c * kernel_size\n",
    "        effective_kernels = non_zero_weights / weights_per_kernel\n",
    "       \n",
    "        # 각 출력 픽셀당 MAC(multiply-accumulate) 연산 수 계산\n",
    "        return batch_size * out_h * out_w * non_zero_weights * 2 \n",
    "\n",
    "    def count_linear_flops(layer):\n",
    "        \"\"\"Linear layer의 실제 FLOPs 계산\"\"\"\n",
    "        # 0이 아닌 weight의 수만 계산\n",
    "        non_zero_weights = torch.count_nonzero(layer.weight).item()\n",
    "        return non_zero_weights * 2  \n",
    "\n",
    "    total_flops = 0\n",
    "    input_shape = (1, 3, 64, 64)\n",
    "   \n",
    "    x_shape = input_shape\n",
    "    for layer in model.features:\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            total_flops += count_conv2d_flops(layer, x_shape)\n",
    "            x_shape = (x_shape[0], layer.out_channels,\n",
    "                      x_shape[2]//layer.stride[0],\n",
    "                      x_shape[3]//layer.stride[1])\n",
    "        elif isinstance(layer, torch.nn.MaxPool2d):\n",
    "            x_shape = (x_shape[0], x_shape[1],\n",
    "                      x_shape[2]//2, x_shape[3]//2)\n",
    "\n",
    "   \n",
    "    for layer in model.classifier:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            total_flops += count_linear_flops(layer)\n",
    "\n",
    "    return {\n",
    "        'params': total_params,\n",
    "        'flops': total_flops\n",
    "    }\n",
    "\n",
    "def gstreamer_pipeline(\n",
    "    sensor_id=0,\n",
    "    capture_width=640,\n",
    "    capture_height=480,\n",
    "    display_width=640,\n",
    "    display_height=480,\n",
    "    framerate=15,\n",
    "    flip_method=0,\n",
    "):\n",
    "    return (\n",
    "        f\"nvarguscamerasrc sensor-id={sensor_id} ! \"\n",
    "        f\"video/x-raw(memory:NVMM), \"\n",
    "        f\"width=(int){capture_width}, height=(int){capture_height}, \"\n",
    "        f\"format=(string)NV12, framerate=(fraction){framerate}/1 ! \"\n",
    "        f\"nvvidconv flip-method={flip_method} ! \"\n",
    "        f\"video/x-raw, width=(int){display_width}, height=(int){display_height}, \"\n",
    "        f\"format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_age_group(age):\n",
    "    \"\"\"연령층 변환 함수\"\"\"\n",
    "    age = int(age)\n",
    "    if age <= 70:\n",
    "        return \"young group\"\n",
    "    else:\n",
    "        return \"old group\"\n",
    "\n",
    "def benchmark_and_demo(model_path, num_samples=1):\n",
    "    \"\"\"실시간 데모와 벤치마크를 동시에 수행\"\"\"\n",
    "    print(f\"\\nRunning demo and benchmark for {os.path.basename(model_path)}...\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    is_student = 'student' in model_path.lower()\n",
    "    model = AgePredictor(is_student=is_student)\n",
    "\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            scale_params = checkpoint.get('scale_params', {'min_age': 0, 'max_age': 100})\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            scale_params = {'min_age': 0, 'max_age': 100}\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        scale_params = {'min_age': 0, 'max_age': 100}\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # 모델 메트릭 계산\n",
    "    metrics = calculate_model_metrics(model)\n",
    "    \n",
    "    # 카메라 및 face detector 설정\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(gstreamer_pipeline(), cv2.CAP_GSTREAMER)\n",
    "    \n",
    "    # FPS 계산을 위한 설정\n",
    "    fps_deque = deque(maxlen=30)\n",
    "    \n",
    "    # 벤치마크 측정을 위한 변수들\n",
    "    processing_times = []\n",
    "    inference_times = []\n",
    "    sample_count = 0\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nRunning real-time demo...\")\n",
    "        print(\"Press SPACE to capture benchmark sample\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # 얼굴 검출\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "            \n",
    "            # FPS 계산\n",
    "            fps_deque.append(time.time())\n",
    "            if len(fps_deque) > 1:\n",
    "                fps = len(fps_deque) / (fps_deque[-1] - fps_deque[0])\n",
    "            else:\n",
    "                fps = 0\n",
    "                \n",
    "            # 실시간 정보 표시\n",
    "            cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Samples: {sample_count}/{num_samples}\", (10, 60),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                face_roi = frame[y:y+h, x:x+w]\n",
    "                face_roi = cv2.resize(face_roi, (64, 64))\n",
    "                face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "                face_array = face_rgb.astype(np.float32) / 255.0\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                face_normalized = (face_array - mean) / std\n",
    "                face_tensor = torch.from_numpy(face_normalized.transpose(2, 0, 1)).float().unsqueeze(0)\n",
    "                \n",
    "                # 나이 예측\n",
    "                inference_start = time.time()\n",
    "                with torch.no_grad():\n",
    "                    prediction = model(face_tensor)\n",
    "                inference_time = time.time() - inference_start\n",
    "                \n",
    "                predicted_age = prediction.item() * (scale_params['max_age'] - \n",
    "                                                     scale_params['min_age']) + scale_params['min_age']\n",
    "                age_group = get_age_group(predicted_age)\n",
    "                \n",
    "                # 결과 표시\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                #cv2.putText(frame, f\"Age: {predicted_age:.1f}\", (x, y-10),\n",
    "                           #cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Group: {age_group}\", (x, y-30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.imshow('Age Prediction', frame)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord(' ') and len(faces) > 0 and sample_count < num_samples:\n",
    "                # 벤치마크 샘플 캡처\n",
    "                process_time = time.time() - start_time\n",
    "                processing_times.append(process_time)\n",
    "                inference_times.append(inference_time)\n",
    "                \n",
    "                sample_count += 1\n",
    "                print(f\"\\nBenchmark sample {sample_count}/{num_samples}:\")\n",
    "                print(f\"  Process Time: {process_time*1000:.1f}ms\")\n",
    "                print(f\"  Inference Time: {inference_time*1000:.1f}ms\")\n",
    "                print(f\"  Age Group: {age_group}\")\n",
    "                \n",
    "                # 캡처 시점 표시\n",
    "                cv2.putText(frame, \"CAPTURED\", (frame.shape[1]//2-60, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                cv2.imshow('Age Prediction', frame)\n",
    "                cv2.waitKey(500)  # 잠시 표시\n",
    "                \n",
    "                if sample_count >= num_samples:\n",
    "                    print(\"\\nBenchmark samples collected!\")\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    if processing_times:\n",
    "        results = {\n",
    "            'model_size': {\n",
    "                'parameters': metrics['params'],\n",
    "                'size_mb': metrics['params'] * 4 / (1024 * 1024)\n",
    "            },\n",
    "            'flops': metrics['flops'],\n",
    "            'speed': {\n",
    "                'average_process_time_ms': np.mean(processing_times) * 1000,\n",
    "                'average_inference_time_ms': np.mean(inference_times) * 1000,\n",
    "                'process_time_std_ms': np.std(processing_times) * 1000,\n",
    "                'inference_time_std_ms': np.std(inference_times) * 1000\n",
    "            }\n",
    "        }\n",
    "        return results\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_comparison(results):\n",
    "    \"\"\"벤치마크 결과 시각화\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 1. 모델 크기\n",
    "    plt.subplot(131)\n",
    "    params = [results[m]['model_size']['parameters'] for m in results]\n",
    "    plt.bar(results.keys(), params)\n",
    "    plt.title('Model Size (Parameters)')\n",
    "    plt.ylabel('Number of Parameters')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 2. FLOPs\n",
    "    plt.subplot(132)\n",
    "    flops = [results[m]['flops'] for m in results]\n",
    "    plt.bar(results.keys(), flops)\n",
    "    plt.title('Computational Cost (FLOPs)')\n",
    "    plt.ylabel('FLOPs')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 3. 실행 속도\n",
    "    plt.subplot(133)\n",
    "    process_times = [results[m]['speed']['average_process_time_ms'] for m in results]\n",
    "    inference_times = [results[m]['speed']['average_inference_time_ms'] for m in results]\n",
    "    \n",
    "    x = np.arange(len(results))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, process_times, width, label='Process Time')\n",
    "    plt.bar(x + width/2, inference_times, width, label='Inference Time')\n",
    "    plt.title('Processing Speed')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.xticks(x, results.keys(), rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison_ts.png')\n",
    "    plt.close()\n",
    "\n",
    "def run_benchmarks():\n",
    "    \"\"\"전체 벤치마크 실행\"\"\"\n",
    "    model_paths = {\n",
    "        'Teacher': 'teacher_model_best (1).pth',\n",
    "        'Student': 'student_model_best (1).pth',\n",
    "        'Pruned (0.3)': 'age_model_pruned_30.pth',\n",
    "        'Pruned (0.5)': 'age_model_pruned_50.pth',\n",
    "        'Pruned (0.7)': 'age_model_pruned_70.pth'\n",
    "        \n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, path in model_paths.items():\n",
    "        print(f\"\\nTesting {name} model...\")\n",
    "        results[name] = benchmark_and_demo(path)\n",
    "        \n",
    "        if results[name]:\n",
    "            print(f\"\\n{name} Model Results:\")\n",
    "            print(f\"Parameters: {results[name]['model_size']['parameters']:,}\")\n",
    "            print(f\"Model Size: {results[name]['model_size']['size_mb']:.2f} MB\")\n",
    "            print(f\"FLOPs: {results[name]['flops']:,}\")\n",
    "            print(f\"Average Process Time: {results[name]['speed']['average_process_time_ms']:.2f} ms\")\n",
    "            print(f\"Average Inference Time: {results[name]['speed']['average_inference_time_ms']:.2f} ms\")\n",
    "            \n",
    "            # 개별 결과 저장\n",
    "            with open(f'results_{name.lower()}_new.json', 'w') as f:\n",
    "                json.dump(results[name], f, indent=4)\n",
    "    \n",
    "    if results:\n",
    "        plot_comparison(results)\n",
    "        \n",
    "        # 전체 결과 저장\n",
    "        with open('benchmark_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Final Comparison Summary\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Model':<10} {'Size(MB)':<10} {'Parameters':<15} {'FLOPs':<15} {'Process(ms)':<15} {'Inference(ms)'}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for name, result in results.items():\n",
    "            print(f\"{name:<10} \"\n",
    "                  f\"{result['model_size']['size_mb']:<10.2f} \"\n",
    "                  f\"{result['model_size']['parameters']:<15,d} \"\n",
    "                  f\"{result['flops']:<15,d} \"\n",
    "                  f\"{result['speed']['average_process_time_ms']:<15.2f} \"\n",
    "                  f\"{result['speed']['average_inference_time_ms']:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time demo and benchmark comparison...\n",
      "\n",
      "Testing Teacher model...\n",
      "\n",
      "Running demo and benchmark for teacher_model_best (1).pth...\n",
      "\n",
      "Running real-time demo...\n",
      "Press SPACE to capture benchmark sample\n",
      "Press 'q' to quit\n",
      "\n",
      "Benchmark sample 1/1:\n",
      "  Process Time: 330.2ms\n",
      "  Inference Time: 47.6ms\n",
      "  Age Group: old group\n",
      "\n",
      "Benchmark samples collected!\n",
      "\n",
      "Teacher Model Results:\n",
      "Parameters: 2,190,913\n",
      "Model Size: 8.36 MB\n",
      "FLOPs: 86,770,176\n",
      "Average Process Time: 330.18 ms\n",
      "Average Inference Time: 47.59 ms\n",
      "\n",
      "Testing Student model...\n",
      "\n",
      "Running demo and benchmark for student_model_best (1).pth...\n",
      "\n",
      "Running real-time demo...\n",
      "Press SPACE to capture benchmark sample\n",
      "Press 'q' to quit\n",
      "\n",
      "Benchmark sample 1/1:\n",
      "  Process Time: 287.1ms\n",
      "  Inference Time: 29.1ms\n",
      "  Age Group: old group\n",
      "\n",
      "Benchmark samples collected!\n",
      "\n",
      "Student Model Results:\n",
      "Parameters: 1,072,673\n",
      "Model Size: 4.09 MB\n",
      "FLOPs: 24,510,976\n",
      "Average Process Time: 287.07 ms\n",
      "Average Inference Time: 29.12 ms\n",
      "\n",
      "================================================================================\n",
      "Final Comparison Summary\n",
      "================================================================================\n",
      "Model      Size(MB)   Parameters      FLOPs           Process(ms)     Inference(ms)\n",
      "--------------------------------------------------------------------------------\n",
      "Teacher    8.36       2,190,913       86,770,176      330.18          47.59\n",
      "Student    4.09       1,072,673       24,510,976      287.07          29.12\n",
      "\n",
      "Benchmark completed! Results saved to files.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting real-time demo and benchmark comparison...\")\n",
    "    results = run_benchmarks()\n",
    "    print(\"\\nBenchmark completed! Results saved to files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
